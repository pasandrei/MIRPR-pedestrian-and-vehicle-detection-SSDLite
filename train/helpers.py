import torch
import numpy as np
import copy

from general_config import anchor_config


def wh2corners_numpy(ctr, wh):
    return np.concatenate([ctr-wh/2, ctr+wh/2], axis=1)


def wh2corners(ctr, wh):
    # taken from fastai
    return torch.cat([ctr-wh/2, ctr+wh/2], dim=1)


def intersect(box_a, box_b):
    # taken from fastai
    """ Returns the intersection of two boxes """
    max_xy = torch.min(box_a[:, None, 2:], box_b[None, :, 2:])
    min_xy = torch.max(box_a[:, None, :2], box_b[None, :, :2])
    inter = torch.clamp((max_xy - min_xy), min=0)
    return inter[:, :, 0] * inter[:, :, 1]


def box_sz(b):
    # taken from fastai
    """ Returns the box size"""
    return ((b[:, 2]-b[:, 0]) * (b[:, 3]-b[:, 1]))


def jaccard(box_a, box_b):
    # taken from fastai
    """ Returns the jaccard distance between two boxes"""
    inter = intersect(box_a, box_b)
    union = box_sz(box_a).unsqueeze(1) + box_sz(box_b).unsqueeze(0) - inter
    return inter / union


def map_to_ground_truth(overlaps, gt_bbox, gt_class, params):
    # taken from fastai
    """ maps priors to max IOU obj
   returns:
   - gt_bbox_for_matched_anchors: tensor of size matched_priors x 4 - essentially assigning GT bboxes to corresponding highest IOU priors
   - matched_gt_class_ids: tensor of size priors - where each value of the tensor indicates the class id that the priors feature map cell should predict
    """

    # for each object, what is the prior of maximum overlap
    gt_to_prior_overlap, gt_to_prior_idx = overlaps.max(1)

    # for each prior, what is the object of maximum overlap
    prior_to_gt_overlap, prior_to_gt_idx = overlaps.max(0)

    # for priors of max overlap, set a high value to make sure they match
    prior_to_gt_overlap[gt_to_prior_idx] = 1.99

    # for each prior, get the actual id of the class it should predict, unmatched anchors (low IOU) should predict background
    matched_gt_class_ids = gt_class[prior_to_gt_idx]
    pos = prior_to_gt_overlap > params.mapping_threshold
    matched_gt_class_ids[~pos] = 100  # background code

    # for each matched prior, get the bbox it should predict
    raw_matched_bbox = gt_bbox[prior_to_gt_idx]
    pos_idx = torch.nonzero(pos)[:, 0]
    # which of those max values are actually precise enough?
    gt_bbox_for_matched_anchors = raw_matched_bbox[pos_idx]

    # so now we have the GT represented with priors
    return gt_bbox_for_matched_anchors, matched_gt_class_ids, pos_idx


def create_anchors():
    # taken from fastai
    '''
    returns in the following format:
    k = zooms * ratios

    A = (grid_size1 ** 2 * k) + (grid_size2 ** 2 * k) +....+ (grid_sizen ** 2 * k) X 4

    where first k lines for this matrix are anchors centered in the top left corner cell of the first grid
    the next k lines are centered in the cell to the right of that (so they are generated by LINES)
    after the first grid is finished comes the next and so on
    '''
    def create(anc_grids, anc_zooms, anc_ratios):
        anchor_scales = [(anz*i, anz*j) for anz in anc_zooms for (i, j) in anc_ratios]
        anc_offsets = [1/(o*2) for o in anc_grids]
        k = len(anchor_scales)

        anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)
                                for ao, ag in zip(anc_offsets, anc_grids)])
        anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)
                                for ao, ag in zip(anc_offsets, anc_grids)])
        anc_ctrs = np.repeat(np.stack([anc_x, anc_y], axis=1), k, axis=0)

        anc_sizes = np.concatenate([np.array([[o/ag, p/ag] for i in range(ag*ag) for o, p in anchor_scales])
                                    for ag in anc_grids])

        grid_sizes = torch.from_numpy(np.concatenate([np.array([1/ag for i in range(ag*ag) for o, p in anchor_scales])
                                                      for ag in anc_grids])).unsqueeze(1)

        anchors = torch.from_numpy(np.concatenate([anc_ctrs, anc_sizes], axis=1)).float()

        return anchors, grid_sizes

    anchors, grid_sizes = [], []
    for (k_zoom, v_zoom), (k_ratio, v_ratio) in zip(anchor_config.zoom.items(), anchor_config.ratio.items()):
        if v_zoom != []:
            cur_anchors, cur_grid_sizes = create([k_zoom], v_zoom, v_ratio)
            anchors.append(cur_anchors)
            grid_sizes.append(cur_grid_sizes)

    anchors = torch.cat(anchors)
    grid_sizes = torch.cat(grid_sizes)

    # are currently (y,x,h,w), want (x,y,w,h)
    temp = copy.deepcopy(anchors[:, 0])
    anchors[:, 0] = anchors[:, 1]
    anchors[:, 1] = temp

    temp = copy.deepcopy(anchors[:, 2])
    anchors[:, 2] = anchors[:, 3]
    anchors[:, 3] = temp

    return anchors, grid_sizes


def prepare_gt(input_img, gt_target):
    '''
    args:
    - input_img: PIL image HxW
    - gt_target: list of gt bbox coords: (x,y,w,h)

    return:
    gt[0] = tensor of bboxes of objects in image scaled [0,1], in (CENTER, w, h) format
    gt[1] = tensor of class ids in image
    '''
    gt_bboxes, gt_classes = [], []
    for obj in gt_target:
        gt_bboxes.append(obj['bbox'])
        gt_classes.append(obj['category_id'])

    gt = [torch.FloatTensor(gt_bboxes), torch.IntTensor(gt_classes)]

    width, height = input_img.size

    for idx, bbox in enumerate(gt[0]):
        new_bbox = [0] * 4
        new_bbox[0] = (bbox[0] + (bbox[2] / 2)) / width
        new_bbox[1] = (bbox[1] + (bbox[3] / 2)) / height
        new_bbox[2] = bbox[2] / width
        new_bbox[3] = bbox[3] / height
        gt[0][idx] = torch.FloatTensor(new_bbox)

    return gt
