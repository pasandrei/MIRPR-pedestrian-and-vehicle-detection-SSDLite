import torch
from torch import nn
import numpy as np
# import torch.nn.functional as F

from train.helpers import *
from misc.postprocessing import *


def test(image, anchors, matched_gt_bbox, matched_pred_bbox, gt_bbox, current_prediction_class_ids, pos_idx):
    '''
    NUMA DOREL STIE CEI ACI
    what we have: - anchors: the set of predifined bounding boxes
                  - gt_bboxes: the ground truth bboxes of objects in the image
                  - using these 2, we want to match those anchors that intersect well with one gt_bbox
                  - DEBUG: plot gt_bboxes and the anchors that have matched to see if this is done correctly
                  - pos_idx is supposed to be the indeces of mapped anchors
                  - matched_gt_bbox is the gt_bbox that each matched anchor has mapped to
    '''

    # # first thing first, get the input tensor ready to be plotted
    # image = (image * 255).cpu().numpy().astype(np.uint8)
    #
    # # same for other variables of interest
    # anchors = (anchors.cpu().numpy() * 320).astype(int)
    # matched_gt_bbox = (matched_gt_bbox.detach().cpu().numpy()*320).astype(int)
    # gt_bbox = (gt_bbox.cpu().numpy() * 320).astype(int)
    # pos_idx = (pos_idx.cpu().numpy())
    # matched_anchors = anchors[pos_idx]

    # first thing first, get the input tensor ready to be plotted
    image = (image * 255).astype(np.uint8)

    # same for other variables of interest
    anchors = (anchors.numpy() * 320).astype(int)
    matched_gt_bbox = (matched_gt_bbox).astype(int)
    gt_bbox = (gt_bbox).astype(int)
    pos_idx = (pos_idx)
    matched_anchors = anchors[pos_idx]

    # print matched anchors, respective gt gt_bboxes
    print("matched ANCHORS: ", matched_anchors, matched_anchors.shape)
    print("Matched BBOXES: ", matched_gt_bbox, matched_gt_bbox.shape)
    print("GT BBOXES: ", gt_bbox, gt_bbox.shape)

    plot_bounding_boxes(image, matched_anchors)
    plot_bounding_boxes(image, gt_bbox)

    '''
    all of the above seems to work well, lets also see what the model predicts from these anchors
    '''
    # matched_pred_bbox = (matched_pred_bbox.detach().cpu().numpy()*320).astype(int)
    matched_pred_bbox = (matched_pred_bbox * 320).astype(int)
    print("Matched Pred BBOXES: ", matched_pred_bbox, matched_pred_bbox.shape)
    print('CONFIDENCES FOR PREDICTED BBOXES: ', current_prediction_class_ids[pos_idx])
    plot_bounding_boxes(image, matched_pred_bbox)

    '''
    yet again, results seem all good, so there is only one more crucial thing to check:
    - are anchors generated C row major style? if not, there is a mismatch between anchors and
    feature map cells
    - this is easy to check, just print out first 100 anchors
    '''
    # first_anchors = anchors[:100]
    # plot_bounding_boxes(image, first_anchors)
    #
    # print("First anchors: ", first_anchors)
    # plot_bounding_boxes(image, first_anchors[6:7])

    '''
    this is a major issue, it seems that anchors are generated by columns, while feature maps,
    when unpacked into a long tensor, this is 99% done line by line, so in this way, the first
    100 lines from the anchors tensor, which are responsible to mapping with objects on the first
    column of an image, are mapped to feature map cells coming from the first LINE of the image. Thus,
    feature map cells are wrongly mapped to parts of the image that their receptive fields
    have nothing to do with => no wonder loss is stagnant
    - the question still remains, how the hell does it work for mr jeremy?
    - one more possibility, openCV takes x and y mathematically, not row and col
    '''
